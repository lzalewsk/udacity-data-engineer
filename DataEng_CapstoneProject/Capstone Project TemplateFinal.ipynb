{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# American Visitors\n",
    "### Udacity Data Engineering Capstone Project  \n",
    "\n",
    "#### Project Summary\n",
    "In this project, where the main goal was to combine what I've learned\n",
    "throughout the program I decided to use datasets included in the Project Workspace. \n",
    "I decided to concentrate on two datasets:\n",
    "- **I94 Immigration Data**: This data comes from the US National Tourism and Trade Office.\n",
    "[Data source](https://travel.trade.gov/research/reports/i94/historical/2016.html)  \n",
    "  I wanted to deal with the processing of a larger set of data provided by Udacity\n",
    "  in `../../data/18-83510-I94-Data-2016/` location of Project Workspace.\n",
    "- **Airport Code Table** - simple table of airport codes and corresponding cities.\n",
    "  Data cames from [this location](https://datahub.io/core/airport-codes#data)\n",
    "  and available in `airport-codes_csv.csv` file.  \n",
    "  \n",
    "additionaly I touch a little **U.S. City Demographic Data** but in case of possible way\n",
    "of extending information in target DWH.   \n",
    "\n",
    "**Project scope**: The main dataset includes data on immigration to the United State. Within this project I transform dataset from SAS format using Spark process and then in Airflow I transforming partitioned and cleaned data from AWS S3 to Redshift Data Warehouse (DWH) for further Business Intelligence analysis.\n",
    "\n",
    "**Use Cases**:  \n",
    "Final solution in AWS Redshift DWH shall allow to answer for example questions:\n",
    "\n",
    "- What are most visited US states for visitors?\n",
    "- How many people visit US in 2016 each month on a business visa?  \n",
    "- How many days in average each visitor stayed in US in each month?  \n",
    "\n",
    "**Project** is devided on two *subprojects*:  \n",
    "\n",
    "- **Jupyter Notebook** where I follow:\n",
    "  - (Step 1) - Identifing and gathering the data  \n",
    "  - (Step 2) - Exploring and Assessing the Data  \n",
    "   In this subproject I use Spark inside Udacity Project Workspace to process I94imigration data\n",
    "    and finally store results in AWS S3 bucket for further processing using Airflow.    \n",
    "- **Airflow** where:\n",
    "  - (Step 3) - Data Model is define and implemented \n",
    "  - (Step 4) - ETL process is implemented   \n",
    "    In this subproject AWS Redshift and Airflow is used.  \n",
    "    More description provided in **Airflow** section below in `README.md` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "import io\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def write_pandas_to_json_on_s3(df, file_dest_name):\n",
    "    \"\"\"\n",
    "    Convert pandas df to json with orient='record' and save data to S3 bucket\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_bucket = s3.Bucket(config['S3']['PROJECT_BUCKET'])\n",
    "    \n",
    "    json_buffer = io.StringIO()\n",
    "    #df.to_json(json_buffer,orient='records')\n",
    "    # Becose of COPY {} FROM JSON in Redshift we nead to prepare one file with multiple JSON records\n",
    "    # array is not supported\n",
    "    json_buffer.write('\\n'.join([json.dumps(row) for row in df.to_dict('records')]))\n",
    "\n",
    "    try:\n",
    "        s3_bucket.put_object(Key=file_dest_name, Body=json_buffer.getvalue())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        \n",
    "def upload_files_to_s3(local_path):\n",
    "    \"\"\"\n",
    "    Uploads all files from `local_path` location to S3 bucket\n",
    "    \"\"\"\n",
    "    # normalize local_path name\n",
    "    local_path = os.path.relpath(local_path)\n",
    "    \n",
    "    # S3 bucket obj def\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_bucket = s3.Bucket(config['S3']['PROJECT_BUCKET'])\n",
    "\n",
    "    # path walk\n",
    "    for subdir, dirs, files in os.walk(local_path):\n",
    "        files = [f for f in files if not f[0] == '.']\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            # print(full_path)\n",
    "            with open(full_path, 'rb') as data:                \n",
    "                s3_bucket.put_object(Key=full_path, Body=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "I have identified following important data mapping for further processing.  \n",
    "\n",
    "![airflow_subdag](./img/file_mapping.png)\n",
    "\n",
    "---\n",
    "### Describe and Gather Data \n",
    "I have define followind steps to do:\n",
    "1. Extract and prepare lookup data witch will be used as Dimensions (from I94_SAS_Labels_Descriptions.SAS file)\n",
    "2. I94 Port data enrichment with data from **Airport Code Table** `airport-codes_csv.csv` including data cleaning and merging\n",
    "3. Preparing data to import by Redshift in JSON format and storing data in AWS S3\n",
    "4. Gethering I94 Imigration data from `../../data/18-83510-I94-Data-2016/` location using PySpark:  \n",
    "    i. cleaning the data  (part of Step 2)  \n",
    "    ii. partitioning by year and mounth and storung parquet format in `sas_data_full` location  \n",
    "5. Send sas_data_full to do S3 (for further analysis using Airflow)\n",
    "\n",
    "**NOTE:** Please check `dwh.cfg` file for proper configure AWS credentials and S3 bucket names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1.1. Extract and prepare lookup data from I94_SAS_Labels_Descriptions.SAS\n",
    "I decided to use sed.  \n",
    "Result are stored in `lookup_data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!echo \"location_id;location_name\"> lookup_data/_i94cntyl.csv\n",
    "!sed -n \"/value i94cnty/,/;/s/\\ \\+\\([0-9]*\\)\\ \\+= \\+'\\(.*\\)'.\\?\\+/\\1;\\2/p\" I94_SAS_Labels_Descriptions.SAS >> lookup_data/_i94cntyl.csv\n",
    "\n",
    "!echo \"mode_id;mode_name\"> lookup_data/_i94model.csv\n",
    "!sed -n \"/i94model/,/;/s/ \\+\\([0-9]*\\).\\?\\+=.\\?\\+'\\(.*\\)'.\\?\\+/\\1;\\2/p\" I94_SAS_Labels_Descriptions.SAS >> lookup_data/_i94model.csv\n",
    "\n",
    "!echo \"port_code;port_name\"> lookup_data/_i94prtl.csv\n",
    "!sed -n \"/value \\$i94prtl/,/;/s/\\ \\+'\\(.*\\)'.\\?\\+=.\\?\\+'\\(.*\\)'/\\1;\\2/p\" I94_SAS_Labels_Descriptions.SAS >> lookup_data/_i94prtl.csv\n",
    "\n",
    "!echo \"address_code;address_name\"> lookup_data/_i94addrl.csv\n",
    "!sed -n \"/value i94addrl/,/;/s/\\t\\+'\\(.*\\)'.\\?\\+=.\\?\\+'\\(.*\\)'.\\?\\+/\\1;\\2/p\" I94_SAS_Labels_Descriptions.SAS >> lookup_data/_i94addrl.csv\n",
    "\n",
    "!echo \"visa_id;visa_category\"> lookup_data/_i94visa.csv\n",
    "!sed -n \"/^\\/\\* I94VISA/,/\\*\\//s/.\\?\\+\\([0-9]\\).\\?\\+= \\?\\+\\(.*\\)/\\1;\\2/p\" I94_SAS_Labels_Descriptions.SAS >> lookup_data/_i94visa.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1.2  - I94Port data enrichment with data from Airport Code Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df_airport = pd.read_csv('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DATA check during DATA EXPLORATION\n",
    "# df_airport.head()\n",
    "# df_airport.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract 'state' from iso_region\n",
    "df_airport['state'] = df_airport['iso_region'].str.split('-',expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split coordinates to Lot,Lat\n",
    "df_airport[['lon','lat']] = df_airport['coordinates'].str.split(',',expand=True)\n",
    "del df_airport['coordinates']\n",
    "# df_airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get port data from SAS lables Description\n",
    "df_i94prt = pd.read_csv('lookup_data/_i94prtl.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split port_name to name and state\n",
    "df_i94prt[['port_name','state']] = df_i94prt['port_name'].str.split(\",\",expand=True)[[0,1]]\n",
    "# remove whitespaces\n",
    "df_i94prt['state'] = df_i94prt['state'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_state_string(s):\n",
    "    \"Simple state cleaning opearation base on manual data check\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    if \"(\" in s:\n",
    "        #print(s)\n",
    "        return s[:2]\n",
    "    if \"#\" in s:\n",
    "        #print(s)\n",
    "        return s.split(\" \")[0]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# apply cleaning\n",
    "df_i94prt['state'] = df_i94prt['state'].apply(clean_state_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Leave only 2-letters data in 'state column, rest in 'info\n",
    "df_i94prt['info'] = df_i94prt[df_i94prt['state'].str.len() > 2]['state']\n",
    "df_i94prt['state'] = df_i94prt[df_i94prt['state'].str.len() <=2]['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check\n",
    "# df_i94prt.groupby('state').count()\n",
    "# df_i94prt.groupby('info').count()\n",
    "\n",
    "# check airport types\n",
    "# df_airport['type'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove closed airports\n",
    "df_airport=df_airport[df_airport['type'] !='closed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define collumns to select\n",
    "airport_collumns = [\n",
    "    'port_code',  # new column from SAS combine base on 'gps_code', 'iata_code', 'local_code'\n",
    "    'ident',    \n",
    "    'type',\n",
    "    'name',\n",
    "    'elevation_ft',\n",
    "    'continent',\n",
    "    'iso_country',\n",
    "    'state',\n",
    "    'municipality',\n",
    "    'lon',\n",
    "    'lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# iterate on 'gps_code', 'iata_code', 'local_code' to find airports from df_i94prt\n",
    "df_airport_final = None\n",
    "# for c in ['gps_code', 'iata_code', 'local_code']: # finaly I have decided to use only gps_code and iata_code\n",
    "for c in ['gps_code', 'iata_code']:\n",
    "    _dfTmp = df_airport[df_airport[c].isin(df_i94prt['port_code'])].copy()\n",
    "    _dfTmp['port_code'] = _dfTmp[c]\n",
    "    _dfTmp = _dfTmp[airport_collumns]\n",
    "    \n",
    "    if df_airport_final is None:\n",
    "        df_airport_final = _dfTmp\n",
    "    else:\n",
    "        df_airport_final = df_airport_final.append(_dfTmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# finaly drop duplicates\n",
    "df_airport_final.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LEFT jonin all 'valid' to i94 SAS port data (base on port and state validation)\n",
    "df_i94prt_final = pd.merge(df_i94prt, df_airport_final, on=['port_code','state'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save final i94port data\n",
    "df_i94prt_final.to_csv('lookup_data/_i94prtl.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_code</th>\n",
       "      <th>port_name</th>\n",
       "      <th>state</th>\n",
       "      <th>info</th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>municipality</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PANC</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>Ted Stevens Anchorage International Airport</td>\n",
       "      <td>152.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>-149.99600219726562</td>\n",
       "      <td>61.174400329589844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "      <td>AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPIZ</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Point Lay LRRS Airport</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>Point Lay</td>\n",
       "      <td>-163.0050049</td>\n",
       "      <td>69.73290253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port_code                 port_name state info ident            type  \\\n",
       "0       ALC                     ALCAN    AK  NaN   NaN             NaN   \n",
       "1       ANC                 ANCHORAGE    AK  NaN  PANC   large_airport   \n",
       "2       BAR  BAKER AAF - BAKER ISLAND    AK  NaN   NaN             NaN   \n",
       "3       DAC             DALTONS CACHE    AK  NaN   NaN             NaN   \n",
       "4       PIZ    DEW STATION PT LAY DEW    AK  NaN  PPIZ  medium_airport   \n",
       "\n",
       "                                          name  elevation_ft continent  \\\n",
       "0                                          NaN           NaN       NaN   \n",
       "1  Ted Stevens Anchorage International Airport         152.0       NaN   \n",
       "2                                          NaN           NaN       NaN   \n",
       "3                                          NaN           NaN       NaN   \n",
       "4                       Point Lay LRRS Airport          22.0       NaN   \n",
       "\n",
       "  iso_country municipality                  lon                  lat  \n",
       "0         NaN          NaN                  NaN                  NaN  \n",
       "1          US    Anchorage  -149.99600219726562   61.174400329589844  \n",
       "2         NaN          NaN                  NaN                  NaN  \n",
       "3         NaN          NaN                  NaN                  NaN  \n",
       "4          US    Point Lay         -163.0050049          69.73290253  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94prt_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1.3. Preparing data to import by Redshift in JSON format and storing data in AWS S3  \n",
    "NOTE: bucket definition is configured in dwh.cfg in [S3] section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing _i94cntyl - done\n",
      "Processing _i94prtl - done\n",
      "Processing _i94model - done\n",
      "Processing _i94addrl - done\n",
      "Processing _i94visa - done\n"
     ]
    }
   ],
   "source": [
    "# read all extracted data from I94_SAS_Labels_Descriptions.SAS and write to S3 in JSON format\n",
    "lookup_data = [\n",
    "    '_i94cntyl',\n",
    "    '_i94prtl',\n",
    "    '_i94model',\n",
    "    '_i94addrl',\n",
    "    '_i94visa'\n",
    "]\n",
    "\n",
    "for n in lookup_data:\n",
    "    df = pd.read_csv(f'lookup_data//{n}.csv',sep=';')\n",
    "    df = df.replace({np.nan:None})\n",
    "    # write to S3 bucket in JSON format supported by AWS Redshift\n",
    "    write_pandas_to_json_on_s3(df,f'lookup_data/{n}.json')\n",
    "    print(f'Processing {n} - done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "NOTE: This data will be used to fill dimension tables i target AMVisitors DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1.4. Exploration of sample data set `immigration_data_sample.csv`\n",
    "Goal: Identification which collumns are usefull and what need to be remove in target PySpark processing\n",
    "NOTE: this section is only for information use to informe about the process of 'DATA EXPLORATION'\n",
    "\n",
    "During this part most of ## Step 2: Explore and Assess the Data were done.\n",
    "Results were implemented in final PySpark ETL process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imigrations = pd.read_csv('immigration_data_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_imigrations.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_imigrations['count'].describe()\n",
    "# count CONCLUSTION: no usefull information, TO_DEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### analysis od data market in I94_SAS_Labels_Descriptions.SAS as \"CIC does not use\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_imigrations[['count',\n",
    "#                 'dtadfile', #CIC does not use\n",
    "#                 'visapost', #CIC does not use\n",
    "#                 'occup',    #CIC does not use\n",
    "#                 'entdepa',  #CIC does not use\n",
    "#                 'entdepd',  #CIC does not use\n",
    "#                 'entdepu',  #CIC does not use\n",
    "#                 'matflag',  #CIC does not use\n",
    "#                 'biryear',  #???\n",
    "#                 'dtaddto',  #CIC does not use\n",
    "#                 'insnum', #????\n",
    "#                 'admnum', #????\n",
    "#             ]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_imigrations[['count',\n",
    "#                 'dtadfile', #CIC does not use\n",
    "#                 'visapost', #CIC does not use\n",
    "#                 'occup',    #CIC does not use\n",
    "#                 'entdepa',  #CIC does not use\n",
    "#                 'entdepd',  #CIC does not use\n",
    "#                 'entdepu',  #CIC does not use\n",
    "#                 'matflag',  #CIC does not use\n",
    "#                 'biryear',  #???\n",
    "#                 'dtaddto',  #CIC does not use\n",
    "#                 'insnum', #????\n",
    "#                 'admnum', #????\n",
    "#             ]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_imigrations[['count',\n",
    "#                 'dtadfile', #CIC does not use\n",
    "#                 'visapost', #CIC does not use\n",
    "#                 'occup',    #CIC does not use\n",
    "#                 'entdepa',  #CIC does not use\n",
    "#                 'entdepd',  #CIC does not use\n",
    "#                 'entdepu',  #CIC does not use\n",
    "#                 'matflag',  #CIC does not use\n",
    "#                 'biryear',  #???\n",
    "#                 'dtaddto',  #CIC does not use\n",
    "#                 'insnum', #????\n",
    "#                 'admnum', #????\n",
    "#             ]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|field|description|\n",
    "|-----|-----------|\n",
    "|cicid|---|\n",
    "|i94yr|4 digit year|\n",
    "|i94mon|Numeric month|\n",
    "|i94cit| ref. to i94cntyl codes (file: i94cntyl.json)|\n",
    "|i94res| ref. to i94cntyl codes (file: i94cntyl.json)|\n",
    "|i94port| ref. to i94prtl codes (file: i94prtl.json)|\n",
    "|arrdate| is the Arrival Date in the USA|\n",
    "|i94mode| ref. to i94model (file: i94model.json). There are missing values as well as not reported (9)|\n",
    "|i94addr| ref. to i94addr codes (file: i94addr.json)|\n",
    "|depdate| is the Departure Date from the USA.|\n",
    "|i94bir|Age of Respondent in Years|\n",
    "|i94visa| ref. to i94visa Visa codes (file: i94visa class)|\n",
    "|count|---|\n",
    "|dtadfile|Character Date Field - Date added to I-94 Files - CIC does not use|\n",
    "|visapost|Department of State where where Visa was issued - CIC does not use|\n",
    "|occup|Occupation that will be performed in U.S. - CIC does not use (a few records|\n",
    "|entdepa|Arrival Flag - admitted or paroled into the U.S. - CIC does not use|\n",
    "|entdepd|Departure Flag - Departed, lost I-94 or is deceased - CIC does not use|\n",
    "|entdepu|Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use|\n",
    "|matflag|Match flag - Match of arrival and departure records|\n",
    "|biryear|4 digit year of birth|\n",
    "|dtaddto|Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use|\n",
    "|gender|Non-immigrant sex|\n",
    "|insnum|INS number|\n",
    "|airline|Airline used to arrive in U.S.|\n",
    "|admnum|Admission Number|\n",
    "|fltno|Flight number of Airline used to arrive in U.S.|\n",
    "|visatype|Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1.5. Gethering I94 Imigration data from `../../data/18-83510-I94-Data-2016/` location using PySpark\n",
    "**NOTE**: main idea is to process I94 data in SAS format, clean and store to sas_data_full folder in parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### PySpark python modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    SparkSession getOrCreate with support of sas7bdat data read.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def convert_sasdate_to_yyyymmdd(sasdate):\n",
    "    if sasdate !=-1:\n",
    "        epoch = datetime.datetime(1960, 1, 1)\n",
    "        return (epoch + datetime.timedelta(days=sasdate)).strftime('%Y%m%d')\n",
    "    else:\n",
    "        epoch = datetime.datetime(1900, 1, 2)\n",
    "        return (epoch + datetime.timedelta(days=sasdate)).strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_sas_data_and_save_to_parquet(df, destination_path):\n",
    "    \"\"\"\n",
    "    Data Process transformation and cleaning on spark DataFrame.\n",
    "    Result are sawed in parquet format in provided location.\n",
    "    Params:\n",
    "    \n",
    "    \"\"\"\n",
    "    # Int columns - DATA CLEAN / Normalization\n",
    "    null_int_cols = {'cicid': -1, \n",
    "                     'i94yr': -1,\n",
    "                     'i94mon': -1,\n",
    "                     'i94cit': 999, #'INVALID: UNKNOWN' from I94_SAS_Labels_Descriptions.SAS\n",
    "                     'i94res': 239, #'INVALID: UNKNOWN COUNTRY' from I94_SAS_Labels_Descriptions.SAS\n",
    "                     'i94mode': 9,  #'Not reported' from  from I94_SAS_Labels_Descriptions.SAS\n",
    "                     'arrdate': -1,\n",
    "                     'depdate': -1,\n",
    "                     'i94bir': -1,\n",
    "                     'i94visa': -1,\n",
    "                     'biryear': -1}\n",
    "\n",
    "    for k in null_int_cols:\n",
    "        df = df.withColumn(k, F.when((F.col(k).isNull()), null_int_cols[k]).otherwise(F.col(k).cast(\"int\")))\n",
    "\n",
    "    # String columns DATA Clean / Normalization\n",
    "    null_str_cols = {'i94port': 'XXX', # NOT REPORTED/UNKNOWN from I94_SAS_Labels_Descriptions.SAS\n",
    "                     'i94addr': '99',  #'All Other Codes' from I94_SAS_Labels_Descriptions.SAS\n",
    "                     'visatype': 'unknown',\n",
    "                     'gender': 'U',    # one letter (M-male,F-female,U-unknown)\n",
    "                     'insnum':'unknown',\n",
    "                     'airline': 'unknown',\n",
    "                     'fltno': 'unknown',\n",
    "                     'occup': 'unknown'}\n",
    "\n",
    "    for k in null_str_cols:\n",
    "        df = df.withColumn(k, F.when((F.col(k).isNull()), null_str_cols[k]).otherwise(F.col(k)))\n",
    "\n",
    "    udf_sasdate_convert = udf(lambda x: convert_sasdate_to_yyyymmdd(x))\n",
    "    df = df.withColumn('arrdate', udf_sasdate_convert(df['arrdate']))\n",
    "    df = df.withColumn('depdate', udf_sasdate_convert(df['depdate']))\n",
    "\n",
    "    # drop unused collumns\n",
    "    # in file I94_SAS_Labels_Descriptions.SAS desc. \"CIC does not use\"\n",
    "    drop_cols = [\n",
    "                    'count',\n",
    "                    'dtadfile', #CIC does not use\n",
    "                    'visapost', #CIC does not use\n",
    "                    'entdepa',  #CIC does not use\n",
    "                    'entdepd',  #CIC does not use\n",
    "                    'entdepu',  #CIC does not use\n",
    "                    'matflag',  #CIC does not use\n",
    "                    'dtaddto',  #CIC does not use\n",
    "                    'validres',    # appears in Month=6    \n",
    "                    'delete_days', # appears in Month=6\n",
    "                    'delete_mexl', # appears in Month=6\n",
    "                    'delete_dup',  # appears in Month=6\n",
    "                    'delete_visa', # appears in Month=6\n",
    "                    'delete_recdup'# appears in Month=6\n",
    "               ]\n",
    "    df = df.drop(*drop_cols)\n",
    "\n",
    "    # admnum - could be usefull so conversion to long\n",
    "    df = df.withColumn(\"admnum\", df['admnum'].cast('Decimal(11,0)').cast(\"long\"))\n",
    "\n",
    "    # Columns for partitioning purpose\n",
    "    df = df.withColumn(\"year\", df[\"i94yr\"])\n",
    "    df = df.withColumn(\"month\", df[\"i94mon\"])\n",
    "\n",
    "    df.write.parquet(destination_path, partitionBy=[\"year\", \"month\"], mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_path_sas_files(path_sas_folder):\n",
    "    \"\"\"\n",
    "    Get full list of sas7bat files in path_sas_folder location\n",
    "    \"\"\"\n",
    "    os.listdir(path_sas_folder)\n",
    "    files_data_sas = []\n",
    "    path_sas_files = [os.path.join(path_sas_folder, file_path) for file_path in os.listdir(path_sas_folder) if\n",
    "                      file_path[-8:] == 'sas7bdat']\n",
    "    return path_sas_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Spark data process RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get array of all sas data files\n",
    "sas_files = get_path_sas_files('../../data/18-83510-I94-Data-2016/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# if we want to process one file\n",
    "# sas_files = sas_files[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for f in sas_files:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.7 ms, sys: 27.9 ms, total: 110 ms\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for f in sas_files:\n",
    "    df_spark =spark.read.format('com.github.saurfang.sas.spark').load(f)\n",
    "    process_sas_data_and_save_to_parquet(df_spark,'sas_data_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_spark =spark.read.parquet('sas_data_full/year=2016/month=4/')\n",
    "df_spark =spark.read.parquet('sas_data_full/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3574989"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spark.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1.5 Send sas_data_full to do S3 (for further analysis using Airflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.21 s, sys: 2.29 s, total: 6.5 s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Upload data to S3 bucket defined in dwh.cfg in S3 section\n",
    "upload_files_to_s3('sas_data_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Optinal Analysis\n",
    "During data exploratio I was wondering how to use US Cities Demographics data.\n",
    "Bellow is part of my study. But finaly (bacouse of time) I decided to not enriched\n",
    "Address dimension form I94 SAS. `df_i94addrl` DataFram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read SAS data addres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cities = pd.read_csv('us-cities-demographics.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State Code</th>\n",
       "      <th>no_of_cities</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>no_of_veterans</th>\n",
       "      <th>avg_household_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>5</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>32.2</td>\n",
       "      <td>764725.0</td>\n",
       "      <td>728750.0</td>\n",
       "      <td>1493475</td>\n",
       "      <td>137460.0</td>\n",
       "      <td>2.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>34</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2448200.0</td>\n",
       "      <td>2715106.0</td>\n",
       "      <td>5163306</td>\n",
       "      <td>352896.0</td>\n",
       "      <td>2.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR</td>\n",
       "      <td>29</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1400724.0</td>\n",
       "      <td>1482165.0</td>\n",
       "      <td>2882889</td>\n",
       "      <td>154390.0</td>\n",
       "      <td>2.526897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>80</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>34.1</td>\n",
       "      <td>11137275.0</td>\n",
       "      <td>11360435.0</td>\n",
       "      <td>22497710</td>\n",
       "      <td>1322525.0</td>\n",
       "      <td>2.774375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>676</td>\n",
       "      <td>California</td>\n",
       "      <td>35.8</td>\n",
       "      <td>61055672.0</td>\n",
       "      <td>62388681.0</td>\n",
       "      <td>123444353</td>\n",
       "      <td>4617022.0</td>\n",
       "      <td>3.095325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State Code  no_of_cities       state  median_age  male_population  \\\n",
       "0         AK             5      Alaska        32.2         764725.0   \n",
       "1         AL            34     Alabama        38.0        2448200.0   \n",
       "2         AR            29    Arkansas        32.6        1400724.0   \n",
       "3         AZ            80     Arizona        34.1       11137275.0   \n",
       "4         CA           676  California        35.8       61055672.0   \n",
       "\n",
       "   female_population  total_population  no_of_veterans  avg_household_size  \n",
       "0           728750.0           1493475        137460.0            2.770000  \n",
       "1          2715106.0           5163306        352896.0            2.430000  \n",
       "2          1482165.0           2882889        154390.0            2.526897  \n",
       "3         11360435.0          22497710       1322525.0            2.774375  \n",
       "4         62388681.0         123444353       4617022.0            3.095325  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define way of aggregation\n",
    "aggr_meth = {\n",
    "    'State Code':'count',\n",
    "    'State':'first',\n",
    "    'Median Age':'median',\n",
    "    'Male Population':'sum',\n",
    "    'Female Population':'sum',\n",
    "    'Total Population':'sum',\n",
    "    'Number of Veterans':'sum',\n",
    "    'Average Household Size':'mean'\n",
    "}\n",
    "# column renaming\n",
    "col_renaming = {\n",
    "    'State Code':'no_of_cities',\n",
    "    'State':'state',\n",
    "    'Median Age':'median_age',\n",
    "    'Male Population':'male_population',\n",
    "    'Female Population':'female_population',\n",
    "    'Total Population':'total_population',\n",
    "    'Number of Veterans':'no_of_veterans',\n",
    "    'Average Household Size':'avg_household_size'\n",
    "}\n",
    "df_states = df_cities.groupby('State Code').agg(aggr_meth).rename(columns=col_renaming).reset_index()\n",
    "df_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get address data from SAS lables Description\n",
    "df_i94addrl = pd.read_csv('lookup_data/_i94addrl.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94addrl_new = pd.merge(df_i94addrl, df_states, left_on='address_code', right_on='State Code')\n",
    "del df_i94addrl_new['State Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address_code</th>\n",
       "      <th>address_name</th>\n",
       "      <th>no_of_cities</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>no_of_veterans</th>\n",
       "      <th>avg_household_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>34</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2448200.0</td>\n",
       "      <td>2715106.0</td>\n",
       "      <td>5163306</td>\n",
       "      <td>352896.0</td>\n",
       "      <td>2.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "      <td>5</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>32.2</td>\n",
       "      <td>764725.0</td>\n",
       "      <td>728750.0</td>\n",
       "      <td>1493475</td>\n",
       "      <td>137460.0</td>\n",
       "      <td>2.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>80</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>34.1</td>\n",
       "      <td>11137275.0</td>\n",
       "      <td>11360435.0</td>\n",
       "      <td>22497710</td>\n",
       "      <td>1322525.0</td>\n",
       "      <td>2.774375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "      <td>29</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1400724.0</td>\n",
       "      <td>1482165.0</td>\n",
       "      <td>2882889</td>\n",
       "      <td>154390.0</td>\n",
       "      <td>2.526897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>676</td>\n",
       "      <td>California</td>\n",
       "      <td>35.8</td>\n",
       "      <td>61055672.0</td>\n",
       "      <td>62388681.0</td>\n",
       "      <td>123444353</td>\n",
       "      <td>4617022.0</td>\n",
       "      <td>3.095325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  address_code address_name  no_of_cities       state  median_age  \\\n",
       "0           AL      ALABAMA            34     Alabama        38.0   \n",
       "1           AK       ALASKA             5      Alaska        32.2   \n",
       "2           AZ      ARIZONA            80     Arizona        34.1   \n",
       "3           AR     ARKANSAS            29    Arkansas        32.6   \n",
       "4           CA   CALIFORNIA           676  California        35.8   \n",
       "\n",
       "   male_population  female_population  total_population  no_of_veterans  \\\n",
       "0        2448200.0          2715106.0           5163306        352896.0   \n",
       "1         764725.0           728750.0           1493475        137460.0   \n",
       "2       11137275.0         11360435.0          22497710       1322525.0   \n",
       "3        1400724.0          1482165.0           2882889        154390.0   \n",
       "4       61055672.0         62388681.0         123444353       4617022.0   \n",
       "\n",
       "   avg_household_size  \n",
       "0            2.430000  \n",
       "1            2.770000  \n",
       "2            2.774375  \n",
       "3            2.526897  \n",
       "4            3.095325  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94addrl_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address_code</th>\n",
       "      <th>address_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  address_code address_name\n",
       "0           AL      ALABAMA\n",
       "1           AK       ALASKA\n",
       "2           AZ      ARIZONA\n",
       "3           AR     ARKANSAS\n",
       "4           CA   CALIFORNIA"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94addrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Base on information I get during Data Exploration I decided to use AWS Redshift DB and Start Schema architecture with staging tables.\n",
    "I decided to use following schema architecure\n",
    "![airflow_subdag](./img/db_schema.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Target ETL processing was implemented in Airflow using following mapping\n",
    "![airflow_subdag](./img/file2tbl_mapping.png)\n",
    "\n",
    "Please see `README.md` file (section **Airflow**) for more details regarding ETL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2. Airflow\n",
    "### 2.1 How to prepare and run this subproject\n",
    "For this task I propose to use DWH implementation based on AWS Redshift.\n",
    "It allows further access for many users (100+) and nice data exploration.\n",
    "Together with Airflow it is good solution for countiouse ETL pipeline proces.   \n",
    "\n",
    "### 2.2 AWS Redshift\n",
    "1. First, edit `dwh.cfg` file. Fill in AWS acces key (KEY) and secret (SECRET).\n",
    "Rest of fields was provided as example. It could be change base on any further needs.\n",
    "2. Prepare AWS Environment with Redshift and proper IAM role to access S3 bucket.\n",
    "- to create AWS Redshift Cluster please run `python aws_env.py -c`\n",
    "- if you want to delete AWS Redshift Cluster please run `python aws_env.py -d`\n",
    "\n",
    "### 2.3 Airflow run\n",
    "In case of this project I decided to use Airflow Project Workspace provided\n",
    "by Udacity. To run this project (after moving project files to Project Workspace)\n",
    "please run following command:`/opt/airflow/start.sh`  \n",
    "![airflow_run](./img/airflow_run.png)\n",
    "\n",
    "### 2.4 Airflow UI\n",
    "1. Prepare Airflow's UI to configure your AWS credentials and connection to Redshift.\n",
    "   * You can use the Project Workspace (from Udacity) and click on the blue Access Airflow button in the bottom right.\n",
    "   * If you'd prefer to run Airflow locally, open http://localhost:8080 in Google Chrome (other browsers occasionally have issues rendering the Airflow UI). \n",
    "2. Create proper Connection in Admin tab\n",
    "   ![airflow_admin](./img/airflow_admin.png)\n",
    "   \n",
    "3. On the create connection page, enter the following values:\n",
    "- Conn Id: Enter aws_credentials.\n",
    "- Conn Type: Enter Amazon Web Services.\n",
    "- Login: Enter your Access key ID from the IAM User credentials.\n",
    "- Password: Enter your Secret access key from the IAM User credentials.\n",
    "![aws_credentials](https://video.udacity-data.com/topher/2019/February/5c5aaefe_connection-aws-credentials/connection-aws-credentials.png)  \n",
    "Once you've entered these values, select Save and Add Another.\n",
    "  \n",
    "4. On the next create connection page, enter the following values:\n",
    "- Conn Id: Enter redshift.  \n",
    "- Conn Type: Enter Postgres.  \n",
    "- Host: Enter the endpoint of your Redshift cluster, excluding the port at the end.  \n",
    "- Schema: Enter dev. This is the Redshift database you want to connect to.  \n",
    "- Login: Enter awsuser.  \n",
    "- Password: Enter the password you created when launching your Redshift cluster.  \n",
    "- Port: Enter 5439.  \n",
    "![redshift](./img/airflow_connection.png)  \n",
    "Once you've entered these values, select Save.\n",
    "\n",
    "### 2.5 AMVisitors DAGs  \n",
    "For the purpose of this project I have prepared two DAGs:  \n",
    "\n",
    "![amv_dags](./img/amvisitors_dags.png)  \n",
    "- **amvisitors_dbschema_dag** - DAG (without schedule) for DB schema initializatin and lookup dimensions Tables filling  \n",
    "  ![amv_dags](./img/DAG_db_schema_creation.png)  \n",
    "  This DAG should by start manualy, each time when there is a need to clean environment\n",
    "  and DB dimension 'lookup table' recreation \n",
    "- **amvisitors_etl_dag** - DAG for processing i94 Imigration data events data @mounthly  \n",
    "  ![amv_dags](./img/DAG_etl.png)  \n",
    "  To start the DAG please switch state from OFF to ON.\n",
    "  After Refresing in Tree View of *amvisitors_etl_dag* status of catching up should be visible.  \n",
    "  ![airflow_dag_tree](./img/DAG_tree.png)  \n",
    "\n",
    "\n",
    "### 2.6 Data Quality Checks  \n",
    "\n",
    "Simple Quality Checks were implemented as part of DAGs in Airflow pipeline on two stages:\n",
    "- after DB Schema creation and Lookup data load\n",
    "- each time DAG is run every month (including catchup process) simple count of adding records are checked.  \n",
    "\n",
    "As example of correct values check after Lookup data load I added two tasks:\n",
    "``` python\n",
    "run_quality_check_dim_port = DataQualityOperator(\n",
    "    task_id='Data_quality_check_dim_port',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    test_sql = \"select lon,lat from dim_port where port_code = 'HOM'\",\n",
    "    test_result = (-151.477,59.6456,)\n",
    ")\n",
    "\n",
    "\n",
    "run_quality_check_dim_address = DataQualityOperator(\n",
    "    task_id='Data_quality_check_dim_address',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    test_sql = \"select address_name from dim_address where address_code = 'FL'\",\n",
    "    test_result = ('FLORIDA',)\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "In production system Quality Check shall be more sophisticated.  \n",
    "\n",
    "### 2.7 Main files\n",
    "The project template includes four files:\n",
    "\n",
    "- `dwh.cfg` main configuration file\n",
    "- `aws_env.py` is where AWSEnv class was implemented to simplify AWS Redshift Cluster creation\n",
    "```  \n",
    "└───airflow                      # Airflow home\n",
    "|   |               \n",
    "│   └───dags                     \n",
    "│   |   │ amvisitors_dbschema_dag.py      # DAG for DB schema creation and lookup table filling\n",
    "|   |   | amvisitors_etl_dag.py           # Main DAG definition\n",
    "|   └───plugins\n",
    "│       │  \n",
    "|       └───helpers\n",
    "|       |   | sql_queries.py     # All sql queries needed for AMVisitorsDB creation and ETL process\n",
    "|       |\n",
    "|       └───operators\n",
    "|       |   | data_quality.py    # DataQualityOperator\n",
    "|       |   | load_dimension.py  # LoadDimensionOperator\n",
    "|       |   | load_fact.py       # LoadFactOperator\n",
    "|       |   | load_lookup.py     # LoadLookupToRedshiftOperator\n",
    "|       |   | stage_redshift.py  # StageToRedshiftOperator\n",
    "```\n",
    "\n",
    "## 3. Database Schema (result of Step 3)\n",
    "Based on Data Exploration and Identification in `Capstone Project TemplateFinal.ipynb` \n",
    "I have proposed following Start DB Schema.\n",
    "\n",
    "![airflow_subdag](./img/db_schema.png)\n",
    "\n",
    "Why did you choose the model you chose?\n",
    "Benefits of Star Schema:\n",
    "* Star schemas are easy for end users and applications to understand and navigate.\n",
    "* Query performance\n",
    "* Load performance and administration\n",
    "* Built-in referential integrity\n",
    "* Easily understood:\n",
    "\n",
    "REMARKS:  \n",
    "1. Becasue and nature of static SAS data description I decided to assume that following\n",
    "   dimensions tables \\[`dim_port`,`dim_mode`,`dim_location`,`dim_address`,`dim_visa`\\]\n",
    "   will be filled ones and during normal ETL process will not be updated.\n",
    "   \n",
    "2. Because of AWS RedShift does not enforce constraints [link](https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html)\n",
    "   there was need to use additional `WHERE <pk_column> NOT IN` clause in `sql_queries.py` for\n",
    "   [`dim_visitor_insert`,`dim_arrdate_time_insert`,`dim_depdate_time_insert`] SQL's.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 4. Project Summary and additional topics (Step 5).  \n",
    "For the purpose of this project I have used:\n",
    "- Jupyter Environment\n",
    "- ETL: Python, Pandas, Pyspark\n",
    "- Data Warehouse: AWS Redshift\n",
    "- Data Pinelines: Apache Airflow\n",
    "- Storage: AWS S3\n",
    "\n",
    "**How often the data should be updated and why?**\n",
    "\n",
    "I think that nowadays data shall be process as soon as possible, event in near real-time\n",
    "but it depends on data collection and transformation from third party data sourcess.  \n",
    "\n",
    "In case of this project and I94 Imigration data - the visitors in each port arrive in and departure at anytime. In production system I think data could be collected with day or even minute precision. It would be efficient to run ETL process hourly or daily.\n",
    "\n",
    "**What IF?:**\n",
    "- If the data was increased by 100x.  \n",
    "  If Redshift DB will be not enought or will be to costly I will considert to use any\n",
    "  sharding DB solution (SQL or event NoSQL) with horizontal scaling.  \n",
    "  \n",
    "- If the pipelines were run on a daily basis by 7am.  \n",
    "  The Airflow scheduler monitors all tasks and all DAGs, and triggers the task instances\n",
    "  whose dependencies have been met.  \n",
    "  \n",
    "- If the database needed to be accessed by 100+ people.  \n",
    "  If we use AWS Redshift there is possible to increase any quatas and limites.  \n",
    "  Plese take a look on [this](https://docs.aws.amazon.com/redshift/latest/mgmt/amazon-redshift-limits.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
